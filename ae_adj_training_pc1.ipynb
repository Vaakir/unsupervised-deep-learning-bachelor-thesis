{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19c9743c",
   "metadata": {},
   "source": [
    "### TRAINING MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "139b02b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT EVERYTHING\n",
    "from Architectures.AE_ADJ_v2 import AE\n",
    "\n",
    "from Data.load import load, load_middle_slices\n",
    "from Visualizations.latent_space_projections import plot_multiple_datasets\n",
    "from Visualizations.plots import plot_middle_slices_in_range, plot_models_training_time, compare_models_loss_history, plot_images, compare_models_reconstruction\n",
    "from Metrics.metrics_tf import MSE_loss, NMSE_loss, NRMSE_loss, SSIM_loss\n",
    "from Metrics.metrics import NMSE, SSIM, NRMSE, MSE\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Input\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import Isomap, TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow.keras import layers, Model, activations, regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import math\n",
    "import nibabel as nib\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc0b19c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Opening faster: 100%|██████████| 3/3 [00:00<00:00, 106.15it/s]\n"
     ]
    }
   ],
   "source": [
    "groups = {}\n",
    "#groups3D = {}\n",
    "for group in tqdm([\"CN\",\"MCI\",\"AD\"],\"Opening faster\"):\n",
    "    groups[group]=np.load(open(f\"Data/D2-{group}.npy\",\"rb\"))\n",
    "    #groups3D[group]=np.load(open(f\"Data/D3-{group}.npy\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "574c1f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreted image shape: (80, 96, 1) Pre-flattened latent shape: (None, 5, 6, 120)\n",
      "Interpreted image shape: (80, 96, 1) Pre-flattened latent shape: (None, 5, 6, 120)\n",
      "Interpreted image shape: (80, 96, 1) Pre-flattened latent shape: (None, 5, 6, 120)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nmodels_list = [\\n    AE(name=\"AE (3h, 32-128f, 16L, Batchnorm, SSIM_loss)\", input_shape=groups[\"CN\"].shape, latent_dim=16,\\n        encoder_layers=[\\n            (layers.Conv2D, 32, (5,5), {\\'strides\\': 2, \\'padding\\': \\'same\\', \\'activation\\': \\'relu\\'}), layers.BatchNormalization(),\\n            (layers.Conv2D, 64, (3,3), {\\'strides\\': 2, \\'padding\\': \\'same\\', \\'activation\\': \\'relu\\'}), layers.BatchNormalization(),\\n            (layers.Conv2D, 128, (3,3), {\\'strides\\': 2, \\'padding\\': \\'same\\', \\'activation\\': \\'relu\\'}), layers.BatchNormalization(),\\n        ],\\n        decoder_layers=[\\n            (layers.Conv2D, 128, (3,3), {\\'padding\\': \\'same\\'}), (layers.UpSampling2D, (2,2)),\\n            (layers.Conv2D, 64,  (3,3), {\\'padding\\': \\'same\\'}), (layers.UpSampling2D, (2,2)),\\n            (layers.Conv2D, 32,  (5,5), {\\'padding\\': \\'same\\'}), (layers.UpSampling2D, (2,2)),\\n            (layers.Conv2D, 1,   (3,3), {\\'padding\\': \\'same\\'}),\\n        ],\\n        VAE_model=True,loss=SSIM_loss\\n)\\n\\ncustom_simplest_model = AE(name=\"custom_simplest_model\", input_shape=H_train2D.shape, latent_dim=16,\\n    encoder_layers=[\\n        (layers.Conv2D, 4, (3,3), {\\'strides\\': 2, \\'padding\\': \\'same\\', \\'activation\\': \\'leaky_relu\\'}),\\n    ],\\n    decoder_layers=[\\n        (layers.Conv2D, 4, (3,3), {\\'padding\\': \\'same\\'}), (layers.UpSampling2D, (2,2)),\\n        (layers.Conv2D, 1, (3,3), {\\'padding\\': \\'same\\'}),\\n    ], VAE_model=False, loss=MSE_loss)\\n\\n\\nalcoholism = AE(name=\"public_alcoholism_paper\", input_shape=H_train2D.shape, latent_dim=16,\\n    encoder_layers=[\\n        (layers.Conv2D, 40, (5, 5), {\\'activation\\': \\'relu\\', \\'strides\\': (3, 3), \\'padding\\': \\'valid\\'}), layers.BatchNormalization(),\\n        (layers.Conv2D, 80, (3, 3), {\\'activation\\': \\'relu\\', \\'strides\\': (5, 5), \\'padding\\': \\'valid\\'}), layers.BatchNormalization(), (layers.MaxPooling2D, {\\'pool_size\\': (3, 3), \\'strides\\': (1, 1), \\'padding\\': \\'same\\'}),\\n        (layers.Conv2D, 120, (3, 3), {\\'activation\\': \\'relu\\', \\'strides\\': (1, 1), \\'padding\\': \\'same\\'}), layers.BatchNormalization(), (layers.MaxPooling2D, {\\'pool_size\\': (3, 3), \\'strides\\': (1, 1), \\'padding\\': \\'same\\'}),\\n        (layers.Conv2D, 120, (3, 3), {\\'activation\\': \\'relu\\', \\'strides\\': (1, 1), \\'padding\\': \\'same\\'}), layers.BatchNormalization(), (layers.MaxPooling2D, {\\'pool_size\\': (3, 3), \\'strides\\': (1, 1), \\'padding\\': \\'same\\'}),\\n    ],\\n    decoder_layers=[\\n        (layers.Conv2D, 120, (3, 3), {\\'activation\\': \\'relu\\', \\'padding\\': \\'same\\'}), layers.BatchNormalization(), (layers.UpSampling2D, {\\'size\\': (2, 2)}),\\n        (layers.Conv2D, 120, (3, 3), {\\'activation\\': \\'relu\\', \\'padding\\': \\'same\\'}), layers.BatchNormalization(), (layers.UpSampling2D, {\\'size\\': (2, 2)}),\\n        (layers.Conv2D, 80, (3, 3), {\\'activation\\': \\'relu\\', \\'padding\\': \\'same\\'}), layers.BatchNormalization(), (layers.UpSampling2D, {\\'size\\': (2, 2)}),\\n        (layers.Conv2D, 80, (3, 3), {\\'activation\\': \\'relu\\', \\'padding\\': \\'same\\'}), layers.BatchNormalization(), (layers.UpSampling2D, {\\'size\\': (2, 2)}),\\n        (layers.Conv2D, 40, (5, 5), {\\'padding\\': \\'same\\'}), \\n        (layers.Conv2D, 1, (3,3), {\\'padding\\': \\'same\\'}),\\n    ],\\n    VAE_model=False, loss=MSE_loss, learning_rate=0.001\\n)\\n\\n\\nfrontiers = AE(name=\"public_frontiers_paper\", input_shape=H_train2D.shape, latent_dim=16,\\n    encoder_layers=[\\n        (layers.Conv2D, 32, (3, 3), {\\'activation\\': \\'relu\\', \\'strides\\': (1, 1), \\'padding\\': \\'same\\'}),\\n        (layers.Conv2D, 32, (3, 3), {\\'activation\\': \\'relu\\', \\'strides\\': (1, 1), \\'padding\\': \\'same\\'}), (layers.MaxPooling2D, {\\'pool_size\\': (2, 2), \\'strides\\': (2, 2), \\'padding\\': \\'same\\'}),\\n        (layers.Conv2D, 32, (3, 3), {\\'activation\\': \\'relu\\', \\'strides\\': (1, 1), \\'padding\\': \\'same\\'}), \\n        (layers.Conv2D, 32, (3, 3), {\\'activation\\': \\'relu\\', \\'strides\\': (2, 2), \\'padding\\': \\'same\\'}), \\n        (layers.Conv2D, 32, (3, 3), {\\'activation\\': \\'relu\\', \\'strides\\': (1, 1), \\'padding\\': \\'same\\'}), \\n    ],\\n    decoder_layers=[\\n        (layers.Conv2DTranspose, 32, (3, 3), {\\'activation\\': \\'relu\\', \\'padding\\': \\'same\\'}),\\n        (layers.Conv2DTranspose, 32, (3, 3), {\\'activation\\': \\'relu\\', \\'padding\\': \\'same\\', \\'strides\\': 2}),\\n        (layers.Conv2DTranspose, 32, (3, 3), {\\'activation\\': \\'relu\\', \\'padding\\': \\'same\\'}), \\n        (layers.Conv2DTranspose, 32, (3, 3), {\\'activation\\': \\'relu\\', \\'padding\\': \\'same\\'}), (layers.UpSampling2D, {\\'size\\': (2, 2)}),\\n        (layers.Conv2DTranspose, 1,  (3, 3), {\\'activation\\': \\'sigmoid\\',\\'padding\\': \\'same\\'}),\\n    ],\\n    VAE_model=False, loss=MSE_loss, learning_rate=0.001\\n)\\n\\n    unsupervised_anomaly = AE(name=\"public_unsupervised_anomaly\", input_shape=H_train2D.shape, latent_dim=128,\\n    encoder_layers=[\\n        (layers.Conv2D, 32, (5, 5), {\\'activation\\': \\'relu\\', \\'strides\\': 2, \\'padding\\': \\'same\\'}), \\n        (layers.Conv2D, 64, (5, 5), {\\'activation\\': \\'relu\\', \\'strides\\': 2, \\'padding\\': \\'same\\'}), \\n        (layers.Conv2D, 128, (5, 5), {\\'activation\\': \\'relu\\', \\'strides\\': 2, \\'padding\\': \\'same\\'}), \\n        (layers.Conv2D, 16, (1, 1), {\\'activation\\': \\'relu\\', \\'strides\\': 1, \\'padding\\': \\'same\\'}), \\n    ],\\n    decoder_layers=[\\n        (layers.Conv2D, 16, (1, 1), {\\'activation\\': \\'relu\\', \\'strides\\': 1, \\'padding\\': \\'same\\'}), \\n        (layers.Conv2D, 128, (5, 5), {\\'activation\\': \\'relu\\', \\'padding\\': \\'same\\'}), (layers.UpSampling2D, {\\'size\\': (2, 2)}),\\n        (layers.Conv2D, 64, (5, 5), {\\'activation\\': \\'relu\\', \\'padding\\': \\'same\\'}), (layers.UpSampling2D, {\\'size\\': (2, 2)}),\\n        (layers.Conv2D, 32, (5, 5), {\\'activation\\': \\'relu\\', \\'padding\\': \\'same\\'}), (layers.UpSampling2D, {\\'size\\': (2, 2)}),\\n        (layers.Conv2D, 1, (1, 1), {\\'activation\\': \\'sigmoid\\', \\'padding\\': \\'same\\'})\\n    ],\\n    VAE_model=False, loss=MSE_loss, learning_rate=0.001\\n)\\n]\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "A2 = AE(name=\"A2\", input_shape=groups[\"CN\"].shape, latent_dim=16,\n",
    "    encoder_layers=[\n",
    "        (layers.Conv2D, 16, (3,3), {'strides': 2, 'padding': 'same', 'activation': 'leaky_relu'}),\n",
    "        (layers.Conv2D, 32, (3,3), {'strides': 2, 'padding': 'same', 'activation': 'leaky_relu'}),\n",
    "    ],\n",
    "    decoder_layers=[\n",
    "        (layers.Conv2D, 32, (3,3), {'padding': 'same'}), (layers.UpSampling2D, (2,2)),\n",
    "        (layers.Conv2D, 16, (3,3), {'padding': 'same'}), (layers.UpSampling2D, (2,2)),\n",
    "        (layers.Conv2D, 1, (3,3), {'padding': 'same'}),\n",
    "    ], VAE_model=False, optimizer=\"adam\", loss_fn=\"mse\").compile_model()\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "models_list = [\n",
    "    AE(name=\"public_repo_\", input_shape=groups[\"CN\"].shape, latent_dim=128,\n",
    "        encoder_layers=[\n",
    "            (layers.Conv2D, 32, (3, 3), {'activation': 'relu', 'padding': 'same'}), layers.BatchNormalization(),\n",
    "            (layers.Conv2D, 32, (3, 3), {'activation': 'relu', 'padding': 'same'}), layers.BatchNormalization(), (layers.MaxPooling2D, {'pool_size': (2, 2)}),\n",
    "            (layers.Conv2D, 64, (3, 3), {'activation': 'relu', 'padding': 'same'}), layers.BatchNormalization(),\n",
    "            (layers.Conv2D, 64, (3, 3), {'activation': 'relu', 'padding': 'same'}), layers.BatchNormalization(), (layers.MaxPooling2D, {'pool_size': (2, 2)}),\n",
    "            (layers.Conv2D, 128, (3, 3), {'activation': 'relu', 'padding': 'same'}), layers.BatchNormalization(),\n",
    "            (layers.Conv2D, 128, (3, 3), {'activation': 'relu', 'padding': 'same'}), layers.BatchNormalization()\n",
    "        ],\n",
    "        decoder_layers=[\n",
    "            (layers.Conv2D, 64, (3, 3), {'activation': 'relu', 'padding': 'same'}), layers.BatchNormalization(),\n",
    "            (layers.Conv2D, 64, (3, 3), {'activation': 'relu', 'padding': 'same'}), layers.BatchNormalization(), (layers.UpSampling2D, {'size': (2, 2)}),\n",
    "            (layers.Conv2D, 32, (3, 3), {'activation': 'relu', 'padding': 'same'}), layers.BatchNormalization(),\n",
    "            (layers.Conv2D, 32, (3, 3), {'activation': 'relu', 'padding': 'same'}), layers.BatchNormalization(), (layers.UpSampling2D, {'size': (2, 2)}),\n",
    "            (layers.Conv2D, 1,  (3, 3), {'activation': 'sigmoid','padding': 'same',}),\n",
    "        ],\n",
    "        VAE_model=False, optimizer=\"adam\", loss_fn=MSE_loss).compile_model(),\n",
    "\"\"\"\n",
    "\n",
    "models_list = [\n",
    "    # AE(name=\"public_alcoholism_paper_2D_mse\", input_shape=groups[\"CN\"].shape, latent_dim=128,\n",
    "    #     encoder_layers=[\n",
    "    #         (layers.Conv2D, 40, (5, 5), {'activation': 'relu', 'strides': (3, 3), 'padding': 'valid'}), layers.BatchNormalization(),\n",
    "    #         (layers.Conv2D, 80, (3, 3), {'activation': 'relu', 'strides': (5, 5), 'padding': 'valid'}), layers.BatchNormalization(), (layers.MaxPooling2D, {'pool_size': (3, 3), 'strides': (1, 1), 'padding': 'same'}),\n",
    "    #         (layers.Conv2D, 120, (3, 3), {'activation': 'relu', 'strides': (1, 1), 'padding': 'same'}), layers.BatchNormalization(), (layers.MaxPooling2D, {'pool_size': (3, 3), 'strides': (1, 1), 'padding': 'same'}),\n",
    "    #         (layers.Conv2D, 120, (3, 3), {'activation': 'relu', 'strides': (1, 1), 'padding': 'same'}), layers.BatchNormalization(), (layers.MaxPooling2D, {'pool_size': (3, 3), 'strides': (1, 1), 'padding': 'same'}),\n",
    "    #     ],\n",
    "    #     decoder_layers=[\n",
    "    #         (layers.Conv2D, 120, (3, 3), {'activation': 'relu', 'padding': 'same'}), layers.BatchNormalization(), (layers.UpSampling2D, {'size': (2, 2)}),\n",
    "    #         (layers.Conv2D, 120, (3, 3), {'activation': 'relu', 'padding': 'same'}), layers.BatchNormalization(), (layers.UpSampling2D, {'size': (2, 2)}),\n",
    "    #         (layers.Conv2D, 80, (3, 3), {'activation': 'relu', 'padding': 'same'}), layers.BatchNormalization(), (layers.UpSampling2D, {'size': (2, 2)}),\n",
    "    #         (layers.Conv2D, 80, (3, 3), {'activation': 'relu', 'padding': 'same'}), layers.BatchNormalization(), (layers.UpSampling2D, {'size': (2, 2)}),\n",
    "    #         (layers.Conv2D, 40, (5, 5), {'padding': 'same'}), \n",
    "    #         (layers.Conv2D, 1, (3,3), {'padding': 'same'}),\n",
    "    #     ], VAE_model=False, optimizer=\"adam\", loss_fn=MSE_loss).compile_model(),\n",
    "    \n",
    "    AE(name=\"public_alcoholism_paper_2D_ssim\", input_shape=groups[\"CN\"].shape, latent_dim=128,\n",
    "        encoder_layers=[\n",
    "            (layers.Conv2D, 40, (5, 5), {'activation': 'relu', 'strides': (3, 3), 'padding': 'valid'}), layers.BatchNormalization(),\n",
    "            (layers.Conv2D, 80, (3, 3), {'activation': 'relu', 'strides': (5, 5), 'padding': 'valid'}), layers.BatchNormalization(), (layers.MaxPooling2D, {'pool_size': (3, 3), 'strides': (1, 1), 'padding': 'same'}),\n",
    "            (layers.Conv2D, 120, (3, 3), {'activation': 'relu', 'strides': (1, 1), 'padding': 'same'}), layers.BatchNormalization(), (layers.MaxPooling2D, {'pool_size': (3, 3), 'strides': (1, 1), 'padding': 'same'}),\n",
    "            (layers.Conv2D, 120, (3, 3), {'activation': 'relu', 'strides': (1, 1), 'padding': 'same'}), layers.BatchNormalization(), (layers.MaxPooling2D, {'pool_size': (3, 3), 'strides': (1, 1), 'padding': 'same'}),\n",
    "        ],\n",
    "        decoder_layers=[\n",
    "            (layers.Conv2D, 120, (3, 3), {'activation': 'relu', 'padding': 'same'}), layers.BatchNormalization(), (layers.UpSampling2D, {'size': (2, 2)}),\n",
    "            (layers.Conv2D, 120, (3, 3), {'activation': 'relu', 'padding': 'same'}), layers.BatchNormalization(), (layers.UpSampling2D, {'size': (2, 2)}),\n",
    "            (layers.Conv2D, 80, (3, 3), {'activation': 'relu', 'padding': 'same'}), layers.BatchNormalization(), (layers.UpSampling2D, {'size': (2, 2)}),\n",
    "            (layers.Conv2D, 80, (3, 3), {'activation': 'relu', 'padding': 'same'}), layers.BatchNormalization(), (layers.UpSampling2D, {'size': (2, 2)}),\n",
    "            (layers.Conv2D, 40, (5, 5), {'padding': 'same'}), \n",
    "            (layers.Conv2D, 1, (3,3), {'padding': 'same'}),\n",
    "        ], VAE_model=False, optimizer=\"adam\", loss_fn=SSIM_loss).compile_model(),\n",
    "    \n",
    "    AE(name=\"public_alcoholism_paper_2D_ssim\", input_shape=groups[\"CN\"].shape, latent_dim=128,\n",
    "        encoder_layers=[\n",
    "            (layers.Conv2D, 40, (5, 5), {'activation': 'relu', 'strides': (3, 3), 'padding': 'valid'}), layers.BatchNormalization(),\n",
    "            (layers.Conv2D, 80, (3, 3), {'activation': 'relu', 'strides': (5, 5), 'padding': 'valid'}), layers.BatchNormalization(), (layers.MaxPooling2D, {'pool_size': (3, 3), 'strides': (1, 1), 'padding': 'same'}),\n",
    "            (layers.Conv2D, 120, (3, 3), {'activation': 'relu', 'strides': (1, 1), 'padding': 'same'}), layers.BatchNormalization(), (layers.MaxPooling2D, {'pool_size': (3, 3), 'strides': (1, 1), 'padding': 'same'}),\n",
    "            (layers.Conv2D, 120, (3, 3), {'activation': 'relu', 'strides': (1, 1), 'padding': 'same'}), layers.BatchNormalization(), (layers.MaxPooling2D, {'pool_size': (3, 3), 'strides': (1, 1), 'padding': 'same'}),\n",
    "        ],\n",
    "        decoder_layers=[\n",
    "            (layers.Conv2D, 120, (3, 3), {'activation': 'relu', 'padding': 'same'}), layers.BatchNormalization(), (layers.UpSampling2D, {'size': (2, 2)}),\n",
    "            (layers.Conv2D, 120, (3, 3), {'activation': 'relu', 'padding': 'same'}), layers.BatchNormalization(), (layers.UpSampling2D, {'size': (2, 2)}),\n",
    "            (layers.Conv2D, 80, (3, 3), {'activation': 'relu', 'padding': 'same'}), layers.BatchNormalization(), (layers.UpSampling2D, {'size': (2, 2)}),\n",
    "            (layers.Conv2D, 80, (3, 3), {'activation': 'relu', 'padding': 'same'}), layers.BatchNormalization(), (layers.UpSampling2D, {'size': (2, 2)}),\n",
    "            (layers.Conv2D, 40, (5, 5), {'padding': 'same'}), \n",
    "            (layers.Conv2D, 1, (3,3), {'padding': 'same'}),\n",
    "        ], VAE_model=True, optimizer=\"adam\", loss_fn=MSE_loss).compile_model(),\n",
    "    \n",
    "    AE(name=\"public_alcoholism_paper_2D_ssim\", input_shape=groups[\"CN\"].shape, latent_dim=128,\n",
    "        encoder_layers=[\n",
    "            (layers.Conv2D, 40, (5, 5), {'activation': 'relu', 'strides': (3, 3), 'padding': 'valid'}), layers.BatchNormalization(),\n",
    "            (layers.Conv2D, 80, (3, 3), {'activation': 'relu', 'strides': (5, 5), 'padding': 'valid'}), layers.BatchNormalization(), (layers.MaxPooling2D, {'pool_size': (3, 3), 'strides': (1, 1), 'padding': 'same'}),\n",
    "            (layers.Conv2D, 120, (3, 3), {'activation': 'relu', 'strides': (1, 1), 'padding': 'same'}), layers.BatchNormalization(), (layers.MaxPooling2D, {'pool_size': (3, 3), 'strides': (1, 1), 'padding': 'same'}),\n",
    "            (layers.Conv2D, 120, (3, 3), {'activation': 'relu', 'strides': (1, 1), 'padding': 'same'}), layers.BatchNormalization(), (layers.MaxPooling2D, {'pool_size': (3, 3), 'strides': (1, 1), 'padding': 'same'}),\n",
    "        ],\n",
    "        decoder_layers=[\n",
    "            (layers.Conv2D, 120, (3, 3), {'activation': 'relu', 'padding': 'same'}), layers.BatchNormalization(), (layers.UpSampling2D, {'size': (2, 2)}),\n",
    "            (layers.Conv2D, 120, (3, 3), {'activation': 'relu', 'padding': 'same'}), layers.BatchNormalization(), (layers.UpSampling2D, {'size': (2, 2)}),\n",
    "            (layers.Conv2D, 80, (3, 3), {'activation': 'relu', 'padding': 'same'}), layers.BatchNormalization(), (layers.UpSampling2D, {'size': (2, 2)}),\n",
    "            (layers.Conv2D, 80, (3, 3), {'activation': 'relu', 'padding': 'same'}), layers.BatchNormalization(), (layers.UpSampling2D, {'size': (2, 2)}),\n",
    "            (layers.Conv2D, 40, (5, 5), {'padding': 'same'}), \n",
    "            (layers.Conv2D, 1, (3,3), {'padding': 'same'}),\n",
    "        ], VAE_model=True, optimizer=\"adam\", loss_fn=SSIM_loss).compile_model()\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "models_list = [\n",
    "    AE(name=\"AE (3h, 32-128f, 16L, Batchnorm, SSIM_loss)\", input_shape=groups[\"CN\"].shape, latent_dim=16,\n",
    "        encoder_layers=[\n",
    "            (layers.Conv2D, 32, (5,5), {'strides': 2, 'padding': 'same', 'activation': 'relu'}), layers.BatchNormalization(),\n",
    "            (layers.Conv2D, 64, (3,3), {'strides': 2, 'padding': 'same', 'activation': 'relu'}), layers.BatchNormalization(),\n",
    "            (layers.Conv2D, 128, (3,3), {'strides': 2, 'padding': 'same', 'activation': 'relu'}), layers.BatchNormalization(),\n",
    "        ],\n",
    "        decoder_layers=[\n",
    "            (layers.Conv2D, 128, (3,3), {'padding': 'same'}), (layers.UpSampling2D, (2,2)),\n",
    "            (layers.Conv2D, 64,  (3,3), {'padding': 'same'}), (layers.UpSampling2D, (2,2)),\n",
    "            (layers.Conv2D, 32,  (5,5), {'padding': 'same'}), (layers.UpSampling2D, (2,2)),\n",
    "            (layers.Conv2D, 1,   (3,3), {'padding': 'same'}),\n",
    "        ],\n",
    "        VAE_model=True,loss=SSIM_loss\n",
    ")\n",
    "\n",
    "custom_simplest_model = AE(name=\"custom_simplest_model\", input_shape=H_train2D.shape, latent_dim=16,\n",
    "    encoder_layers=[\n",
    "        (layers.Conv2D, 4, (3,3), {'strides': 2, 'padding': 'same', 'activation': 'leaky_relu'}),\n",
    "    ],\n",
    "    decoder_layers=[\n",
    "        (layers.Conv2D, 4, (3,3), {'padding': 'same'}), (layers.UpSampling2D, (2,2)),\n",
    "        (layers.Conv2D, 1, (3,3), {'padding': 'same'}),\n",
    "    ], VAE_model=False, loss=MSE_loss)\n",
    "\n",
    "\n",
    "alcoholism = AE(name=\"public_alcoholism_paper\", input_shape=H_train2D.shape, latent_dim=16,\n",
    "    encoder_layers=[\n",
    "        (layers.Conv2D, 40, (5, 5), {'activation': 'relu', 'strides': (3, 3), 'padding': 'valid'}), layers.BatchNormalization(),\n",
    "        (layers.Conv2D, 80, (3, 3), {'activation': 'relu', 'strides': (5, 5), 'padding': 'valid'}), layers.BatchNormalization(), (layers.MaxPooling2D, {'pool_size': (3, 3), 'strides': (1, 1), 'padding': 'same'}),\n",
    "        (layers.Conv2D, 120, (3, 3), {'activation': 'relu', 'strides': (1, 1), 'padding': 'same'}), layers.BatchNormalization(), (layers.MaxPooling2D, {'pool_size': (3, 3), 'strides': (1, 1), 'padding': 'same'}),\n",
    "        (layers.Conv2D, 120, (3, 3), {'activation': 'relu', 'strides': (1, 1), 'padding': 'same'}), layers.BatchNormalization(), (layers.MaxPooling2D, {'pool_size': (3, 3), 'strides': (1, 1), 'padding': 'same'}),\n",
    "    ],\n",
    "    decoder_layers=[\n",
    "        (layers.Conv2D, 120, (3, 3), {'activation': 'relu', 'padding': 'same'}), layers.BatchNormalization(), (layers.UpSampling2D, {'size': (2, 2)}),\n",
    "        (layers.Conv2D, 120, (3, 3), {'activation': 'relu', 'padding': 'same'}), layers.BatchNormalization(), (layers.UpSampling2D, {'size': (2, 2)}),\n",
    "        (layers.Conv2D, 80, (3, 3), {'activation': 'relu', 'padding': 'same'}), layers.BatchNormalization(), (layers.UpSampling2D, {'size': (2, 2)}),\n",
    "        (layers.Conv2D, 80, (3, 3), {'activation': 'relu', 'padding': 'same'}), layers.BatchNormalization(), (layers.UpSampling2D, {'size': (2, 2)}),\n",
    "        (layers.Conv2D, 40, (5, 5), {'padding': 'same'}), \n",
    "        (layers.Conv2D, 1, (3,3), {'padding': 'same'}),\n",
    "    ],\n",
    "    VAE_model=False, loss=MSE_loss, learning_rate=0.001\n",
    ")\n",
    "\n",
    "\n",
    "frontiers = AE(name=\"public_frontiers_paper\", input_shape=H_train2D.shape, latent_dim=16,\n",
    "    encoder_layers=[\n",
    "        (layers.Conv2D, 32, (3, 3), {'activation': 'relu', 'strides': (1, 1), 'padding': 'same'}),\n",
    "        (layers.Conv2D, 32, (3, 3), {'activation': 'relu', 'strides': (1, 1), 'padding': 'same'}), (layers.MaxPooling2D, {'pool_size': (2, 2), 'strides': (2, 2), 'padding': 'same'}),\n",
    "        (layers.Conv2D, 32, (3, 3), {'activation': 'relu', 'strides': (1, 1), 'padding': 'same'}), \n",
    "        (layers.Conv2D, 32, (3, 3), {'activation': 'relu', 'strides': (2, 2), 'padding': 'same'}), \n",
    "        (layers.Conv2D, 32, (3, 3), {'activation': 'relu', 'strides': (1, 1), 'padding': 'same'}), \n",
    "    ],\n",
    "    decoder_layers=[\n",
    "        (layers.Conv2DTranspose, 32, (3, 3), {'activation': 'relu', 'padding': 'same'}),\n",
    "        (layers.Conv2DTranspose, 32, (3, 3), {'activation': 'relu', 'padding': 'same', 'strides': 2}),\n",
    "        (layers.Conv2DTranspose, 32, (3, 3), {'activation': 'relu', 'padding': 'same'}), \n",
    "        (layers.Conv2DTranspose, 32, (3, 3), {'activation': 'relu', 'padding': 'same'}), (layers.UpSampling2D, {'size': (2, 2)}),\n",
    "        (layers.Conv2DTranspose, 1,  (3, 3), {'activation': 'sigmoid','padding': 'same'}),\n",
    "    ],\n",
    "    VAE_model=False, loss=MSE_loss, learning_rate=0.001\n",
    ")\n",
    "\n",
    "    unsupervised_anomaly = AE(name=\"public_unsupervised_anomaly\", input_shape=H_train2D.shape, latent_dim=128,\n",
    "    encoder_layers=[\n",
    "        (layers.Conv2D, 32, (5, 5), {'activation': 'relu', 'strides': 2, 'padding': 'same'}), \n",
    "        (layers.Conv2D, 64, (5, 5), {'activation': 'relu', 'strides': 2, 'padding': 'same'}), \n",
    "        (layers.Conv2D, 128, (5, 5), {'activation': 'relu', 'strides': 2, 'padding': 'same'}), \n",
    "        (layers.Conv2D, 16, (1, 1), {'activation': 'relu', 'strides': 1, 'padding': 'same'}), \n",
    "    ],\n",
    "    decoder_layers=[\n",
    "        (layers.Conv2D, 16, (1, 1), {'activation': 'relu', 'strides': 1, 'padding': 'same'}), \n",
    "        (layers.Conv2D, 128, (5, 5), {'activation': 'relu', 'padding': 'same'}), (layers.UpSampling2D, {'size': (2, 2)}),\n",
    "        (layers.Conv2D, 64, (5, 5), {'activation': 'relu', 'padding': 'same'}), (layers.UpSampling2D, {'size': (2, 2)}),\n",
    "        (layers.Conv2D, 32, (5, 5), {'activation': 'relu', 'padding': 'same'}), (layers.UpSampling2D, {'size': (2, 2)}),\n",
    "        (layers.Conv2D, 1, (1, 1), {'activation': 'sigmoid', 'padding': 'same'})\n",
    "    ],\n",
    "    VAE_model=False, loss=MSE_loss, learning_rate=0.001\n",
    ")\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0556c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training public_alcoholism_paper_2D_ssim on CPU\n",
      "Input shape: (988, 80, 96)\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2s/step - loss: 0.9630 - val_loss: 0.8415\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2s/step - loss: 0.7993 - val_loss: 0.7001\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - loss: 0.6598 - val_loss: 0.5675\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - loss: 0.5630 - val_loss: 0.5120\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3s/step - loss: 0.5211 - val_loss: 0.4709\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 3s/step - loss: 0.4949 - val_loss: 0.4559\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3s/step - loss: 0.4628 - val_loss: 0.4408\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - loss: 0.4479 - val_loss: 0.4375\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3s/step - loss: 0.4425 - val_loss: 0.4265\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3s/step - loss: 0.4312 - val_loss: 0.4194\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3s/step - loss: 0.4252 - val_loss: 0.4106\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - loss: 0.4230 - val_loss: 0.4089\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3s/step - loss: 0.4195 - val_loss: 0.4066\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3s/step - loss: 0.4174 - val_loss: 0.4068\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3s/step - loss: 0.4189 - val_loss: 0.4061\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 3s/step - loss: 0.4167 - val_loss: 0.4044\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3s/step - loss: 0.4160 - val_loss: 0.4046\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3s/step - loss: 0.4158 - val_loss: 0.4040\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3s/step - loss: 0.4145 - val_loss: 0.4061\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3s/step - loss: 0.4163 - val_loss: 0.4065\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - loss: 0.4159 - val_loss: 0.4031\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3s/step - loss: 0.4156 - val_loss: 0.4041\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3s/step - loss: 0.4153 - val_loss: 0.4042\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3s/step - loss: 0.4154 - val_loss: 0.4031\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3s/step - loss: 0.4125 - val_loss: 0.4023\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3s/step - loss: 0.4160 - val_loss: 0.4045\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3s/step - loss: 0.4160 - val_loss: 0.4037\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3s/step - loss: 0.4152 - val_loss: 0.4039\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3s/step - loss: 0.4137 - val_loss: 0.4028\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3s/step - loss: 0.4142 - val_loss: 0.4029\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3s/step - loss: 0.4128 - val_loss: 0.4025\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3s/step - loss: 0.4137 - val_loss: 0.4035\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 4s/step - loss: 0.4140 - val_loss: 0.4029\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 4s/step - loss: 0.4155 - val_loss: 0.4026\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 3s/step - loss: 0.4130 - val_loss: 0.4032\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 3s/step - loss: 0.4143 - val_loss: 0.4015\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3s/step - loss: 0.4153 - val_loss: 0.4015\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - loss: 0.4112 - val_loss: 0.4032\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - loss: 0.4129 - val_loss: 0.4042\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - loss: 0.4143 - val_loss: 0.4026\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - loss: 0.4128 - val_loss: 0.4036\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - loss: 0.4148 - val_loss: 0.4012\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - loss: 0.4145 - val_loss: 0.4007\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - loss: 0.4117 - val_loss: 0.4184\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - loss: 0.4203 - val_loss: 0.4076\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - loss: 0.4127 - val_loss: 0.4015\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - loss: 0.4078 - val_loss: 0.3974\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3s/step - loss: 0.4073 - val_loss: 0.3936\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3s/step - loss: 0.4049 - val_loss: 0.3923\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - loss: 0.3998 - val_loss: 0.3905\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - loss: 0.3985 - val_loss: 0.3916\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3s/step - loss: 0.4010 - val_loss: 0.3899\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - loss: 0.3965 - val_loss: 0.3885\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - loss: 0.3915 - val_loss: 0.3899\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - loss: 0.3945 - val_loss: 0.3918\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - loss: 0.3925 - val_loss: 0.3864\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - loss: 0.3891 - val_loss: 0.3872\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - loss: 0.3851 - val_loss: 0.3852\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3s/step - loss: 0.3854 - val_loss: 0.3870\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 4s/step - loss: 0.3858 - val_loss: 0.3855\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 3s/step - loss: 0.3818 - val_loss: 0.3817\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3s/step - loss: 0.3799 - val_loss: 0.3812\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 3s/step - loss: 0.3781 - val_loss: 0.3797\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3s/step - loss: 0.3764 - val_loss: 0.3765\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3s/step - loss: 0.3717 - val_loss: 0.3757\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 4s/step - loss: 0.3715 - val_loss: 0.3766\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 3s/step - loss: 0.3708 - val_loss: 0.3725\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - loss: 0.3694 - val_loss: 0.3729\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - loss: 0.3667 - val_loss: 0.3723\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3s/step - loss: 0.3641 - val_loss: 0.3726\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3s/step - loss: 0.3631 - val_loss: 0.3707\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3s/step - loss: 0.3598 - val_loss: 0.3710\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3s/step - loss: 0.3576 - val_loss: 0.3703\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3s/step - loss: 0.3572 - val_loss: 0.3715\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - loss: 0.3530 - val_loss: 0.3714\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3s/step - loss: 0.3502 - val_loss: 0.3659\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3s/step - loss: 0.3469 - val_loss: 0.3665\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - loss: 0.3435 - val_loss: 0.3671\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3s/step - loss: 0.3443 - val_loss: 0.3672\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3s/step - loss: 0.3421 - val_loss: 0.3681\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - loss: 0.3399 - val_loss: 0.3692\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - loss: 0.3366 - val_loss: 0.3653\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - loss: 0.3349 - val_loss: 0.3671\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - loss: 0.3299 - val_loss: 0.3651\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3s/step - loss: 0.3292 - val_loss: 0.3634\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3s/step - loss: 0.3260 - val_loss: 0.3657\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - loss: 0.3257 - val_loss: 0.3664\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3s/step - loss: 0.3240 - val_loss: 0.3671\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - loss: 0.3202 - val_loss: 0.3660\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3s/step - loss: 0.3179 - val_loss: 0.3667\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3s/step - loss: 0.3142 - val_loss: 0.3667\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3s/step - loss: 0.3094 - val_loss: 0.3671\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - loss: 0.3092 - val_loss: 0.3677\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - loss: 0.3083 - val_loss: 0.3737\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - loss: 0.3059 - val_loss: 0.3694\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - loss: 0.3007 - val_loss: 0.3693\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - loss: 0.2978 - val_loss: 0.3684\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - loss: 0.2940 - val_loss: 0.3703\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - loss: 0.2887 - val_loss: 0.3715\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3s/step - loss: 0.2881 - val_loss: 0.3791\n",
      "Models saved to C:\\Users\\kiran\\Documents\\_UIS\\sem6\\BACH\\DementiaMRI\\AE_ADJ_models\\newer_images\\public_alcoholism_paper_2D_ssim\\epoch_100 as 'public_alcoholism_paper_2D_ssim_autoencoder.keras', 'public_alcoholism_paper_2D_ssim_encoder.keras', 'public_alcoholism_paper_2D_ssim_decoder.keras'.\n",
      "Loss history saved to C:\\Users\\kiran\\Documents\\_UIS\\sem6\\BACH\\DementiaMRI\\AE_ADJ_models\\newer_images\\public_alcoholism_paper_2D_ssim\\epoch_100\\history.json\n",
      "Model details saved to C:\\Users\\kiran\\Documents\\_UIS\\sem6\\BACH\\DementiaMRI\\AE_ADJ_models\\newer_images\\public_alcoholism_paper_2D_ssim\\epoch_100\\configs.json\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - loss: 0.2880 - val_loss: 0.3756\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - loss: 0.2852 - val_loss: 0.3730\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - loss: 0.2797 - val_loss: 0.3728\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - loss: 0.2756 - val_loss: 0.3781\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3s/step - loss: 0.2731 - val_loss: 0.3742\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - loss: 0.2701 - val_loss: 0.3752\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3s/step - loss: 0.2661 - val_loss: 0.3791\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3s/step - loss: 0.2679 - val_loss: 0.3798\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3s/step - loss: 0.2634 - val_loss: 0.3795\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3s/step - loss: 0.2556 - val_loss: 0.3783\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3s/step - loss: 0.2532 - val_loss: 0.3814\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - loss: 0.2516 - val_loss: 0.3827\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 0.2518 - val_loss: 0.3823\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 0.2456 - val_loss: 0.3802\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.2427 - val_loss: 0.3849\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 0.2384 - val_loss: 0.3820\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 0.2354 - val_loss: 0.3813\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 0.2317 - val_loss: 0.3851\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 0.2299 - val_loss: 0.3841\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 0.2277 - val_loss: 0.3849\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 0.2277 - val_loss: 0.3848\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 0.2300 - val_loss: 0.3899\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 0.2237 - val_loss: 0.3873\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 0.2207 - val_loss: 0.3868\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 0.2165 - val_loss: 0.3870\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 0.2133 - val_loss: 0.3899\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 0.2108 - val_loss: 0.3903\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 0.2093 - val_loss: 0.3908\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 0.2069 - val_loss: 0.3902\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 0.2064 - val_loss: 0.3922\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 0.2046 - val_loss: 0.3883\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.2017 - val_loss: 0.3918\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2s/step - loss: 0.2001 - val_loss: 0.3914\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - loss: 0.1958 - val_loss: 0.3915\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1942 - val_loss: 0.3942\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1921 - val_loss: 0.3940\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 0.1883 - val_loss: 0.3953\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - loss: 0.1875 - val_loss: 0.3955\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 0.1881 - val_loss: 0.3960\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 0.1866 - val_loss: 0.3973\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 0.1873 - val_loss: 0.3993\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - loss: 0.1845 - val_loss: 0.3986\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1817 - val_loss: 0.3959\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1808 - val_loss: 0.3971\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1804 - val_loss: 0.4017\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1791 - val_loss: 0.3953\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1773 - val_loss: 0.3998\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1746 - val_loss: 0.3982\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1730 - val_loss: 0.3987\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1702 - val_loss: 0.3974\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1676 - val_loss: 0.3980\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1674 - val_loss: 0.3966\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1657 - val_loss: 0.4014\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1652 - val_loss: 0.4042\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 0.1668 - val_loss: 0.4028\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1659 - val_loss: 0.4022\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1676 - val_loss: 0.4052\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1726 - val_loss: 0.4032\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1675 - val_loss: 0.4025\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1673 - val_loss: 0.4046\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1652 - val_loss: 0.3982\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1641 - val_loss: 0.4063\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1601 - val_loss: 0.4012\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1590 - val_loss: 0.4044\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1558 - val_loss: 0.4027\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1550 - val_loss: 0.4010\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1539 - val_loss: 0.4071\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1521 - val_loss: 0.4039\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1516 - val_loss: 0.4035\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1499 - val_loss: 0.4061\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1495 - val_loss: 0.4057\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1482 - val_loss: 0.4041\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1474 - val_loss: 0.4071\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1481 - val_loss: 0.4032\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1477 - val_loss: 0.4050\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1478 - val_loss: 0.4098\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1488 - val_loss: 0.4098\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1475 - val_loss: 0.4065\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1472 - val_loss: 0.4054\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1451 - val_loss: 0.4054\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1443 - val_loss: 0.4075\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1441 - val_loss: 0.4088\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1425 - val_loss: 0.4052\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1405 - val_loss: 0.4053\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1403 - val_loss: 0.4099\n",
      "Early stopping at epoch 185 due to no improvement in val_loss for 100 epochs.\n",
      "Models saved to C:\\Users\\kiran\\Documents\\_UIS\\sem6\\BACH\\DementiaMRI\\AE_ADJ_models\\newer_images\\public_alcoholism_paper_2D_ssim\\epoch_185 as 'public_alcoholism_paper_2D_ssim_autoencoder.keras', 'public_alcoholism_paper_2D_ssim_encoder.keras', 'public_alcoholism_paper_2D_ssim_decoder.keras'.\n",
      "Loss history saved to C:\\Users\\kiran\\Documents\\_UIS\\sem6\\BACH\\DementiaMRI\\AE_ADJ_models\\newer_images\\public_alcoholism_paper_2D_ssim\\epoch_185\\history.json\n",
      "Model details saved to C:\\Users\\kiran\\Documents\\_UIS\\sem6\\BACH\\DementiaMRI\\AE_ADJ_models\\newer_images\\public_alcoholism_paper_2D_ssim\\epoch_185\\configs.json\n",
      "Training complete in 3327.16s\n",
      "Training public_alcoholism_paper_2D_ssim on CPU\n",
      "Input shape: (988, 80, 96)\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2s/step - loss: 4543.9668 - val_loss: 3176.0671\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3749.2175 - val_loss: 3102.6677\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3681.1704 - val_loss: 3132.1023\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3657.7134 - val_loss: 3102.0776\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3710.5874 - val_loss: 3055.6099\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3655.9409 - val_loss: 3075.2424\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3653.2966 - val_loss: 3121.1614\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3695.6836 - val_loss: 3066.4060\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3669.4019 - val_loss: 3068.9221\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3671.3989 - val_loss: 3131.5669\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3697.3391 - val_loss: 3101.2974\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3661.9531 - val_loss: 3116.3850\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3691.0044 - val_loss: 3172.6882\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3685.5720 - val_loss: 3131.8467\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3695.4050 - val_loss: 3076.2749\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3621.4556 - val_loss: 3111.7588\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3669.4543 - val_loss: 3114.8918\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3659.5657 - val_loss: 3065.6091\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3703.8835 - val_loss: 3103.9204\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3660.6340 - val_loss: 3127.4097\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3677.6848 - val_loss: 3090.2756\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3643.3350 - val_loss: 3087.5730\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2s/step - loss: 3657.0356 - val_loss: 3091.8501\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3669.5649 - val_loss: 3105.3423\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3694.0234 - val_loss: 3116.0564\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3662.2114 - val_loss: 3101.0745\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3706.4917 - val_loss: 3057.3958\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3689.6653 - val_loss: 3098.4224\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3647.5178 - val_loss: 3112.5481\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3666.6597 - val_loss: 3130.0613\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3652.3381 - val_loss: 3157.2336\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2s/step - loss: 3628.0916 - val_loss: 3073.3440\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - loss: 3672.0859 - val_loss: 3042.8269\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3671.7620 - val_loss: 3162.9243\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3674.2751 - val_loss: 3145.4407\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3701.1274 - val_loss: 3096.4807\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3701.4771 - val_loss: 3097.5461\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3641.1743 - val_loss: 3117.4712\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3687.6650 - val_loss: 3131.9399\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - loss: 3673.1128 - val_loss: 3119.0537\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3647.3472 - val_loss: 3073.5681\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3677.9099 - val_loss: 3141.6226\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3666.5706 - val_loss: 3099.8667\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3666.0803 - val_loss: 3115.0681\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3677.1672 - val_loss: 3109.0481\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3706.5088 - val_loss: 3079.1726\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3683.2097 - val_loss: 3160.2190\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - loss: 3692.9893 - val_loss: 3130.8816\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3625.7729 - val_loss: 3100.2024\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3685.1655 - val_loss: 3096.5444\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3666.7236 - val_loss: 3140.3469\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - loss: 3664.5581 - val_loss: 3144.4292\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - loss: 3649.8477 - val_loss: 3094.8101\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3679.3718 - val_loss: 3121.1426\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - loss: 3665.8936 - val_loss: 3103.0789\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - loss: 3676.3074 - val_loss: 3160.5872\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - loss: 3613.5271 - val_loss: 3116.7307\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - loss: 3654.5217 - val_loss: 3099.0735\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3669.1270 - val_loss: 3088.3418\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - loss: 3625.9504 - val_loss: 3111.9741\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3671.7698 - val_loss: 3094.6357\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3654.6636 - val_loss: 3039.1350\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3678.5708 - val_loss: 3054.3555\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3662.0659 - val_loss: 3054.6921\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3647.9585 - val_loss: 3118.6812\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3648.4224 - val_loss: 3079.5659\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3644.9939 - val_loss: 3191.4312\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3638.8987 - val_loss: 3070.2336\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3678.2334 - val_loss: 3126.4031\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3676.5483 - val_loss: 3122.7014\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3696.0583 - val_loss: 3095.9668\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3667.1799 - val_loss: 3068.7310\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3652.8196 - val_loss: 3101.6731\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3689.5308 - val_loss: 3136.0796\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3679.4648 - val_loss: 3067.0327\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3657.1553 - val_loss: 3066.8677\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3644.3164 - val_loss: 3121.6672\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3682.6672 - val_loss: 3129.9443\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3656.8926 - val_loss: 3098.4644\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3665.6216 - val_loss: 3107.7864\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3707.2529 - val_loss: 3101.9475\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3667.7544 - val_loss: 3100.8191\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3699.2451 - val_loss: 3103.5107\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3642.0659 - val_loss: 3103.1445\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3648.9536 - val_loss: 3105.9761\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3662.2708 - val_loss: 3138.4824\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3663.2876 - val_loss: 3143.0667\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3708.1880 - val_loss: 3134.3330\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3635.3420 - val_loss: 3090.3699\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3664.5056 - val_loss: 3119.4624\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3670.7537 - val_loss: 3116.2805\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3659.0671 - val_loss: 3104.1819\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3684.8569 - val_loss: 3156.4966\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3697.0627 - val_loss: 3133.9246\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3668.2144 - val_loss: 3048.3914\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3679.8333 - val_loss: 3115.8364\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3639.0598 - val_loss: 3125.0413\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3722.8994 - val_loss: 3105.8806\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3673.2048 - val_loss: 3115.0903\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3684.7217 - val_loss: 3104.7188\n",
      "Models saved to C:\\Users\\kiran\\Documents\\_UIS\\sem6\\BACH\\DementiaMRI\\AE_ADJ_models\\newer_images\\public_alcoholism_paper_2D_ssim\\epoch_100 as 'public_alcoholism_paper_2D_ssim_autoencoder.keras', 'public_alcoholism_paper_2D_ssim_encoder.keras', 'public_alcoholism_paper_2D_ssim_decoder.keras'.\n",
      "Loss history saved to C:\\Users\\kiran\\Documents\\_UIS\\sem6\\BACH\\DementiaMRI\\AE_ADJ_models\\newer_images\\public_alcoholism_paper_2D_ssim\\epoch_100\\history.json\n",
      "Model details saved to C:\\Users\\kiran\\Documents\\_UIS\\sem6\\BACH\\DementiaMRI\\AE_ADJ_models\\newer_images\\public_alcoholism_paper_2D_ssim\\epoch_100\\configs.json\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3660.3687 - val_loss: 3105.1509\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3659.4858 - val_loss: 3144.2964\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3671.2688 - val_loss: 3085.1624\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3657.6531 - val_loss: 3066.3672\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3663.2214 - val_loss: 3097.3435\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3655.8325 - val_loss: 3126.4441\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3691.7791 - val_loss: 3088.1829\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3656.9265 - val_loss: 3058.2725\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3690.2075 - val_loss: 3098.5481\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3646.5776 - val_loss: 3125.9858\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3639.0896 - val_loss: 3108.0757\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3645.5723 - val_loss: 3123.6123\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3685.8118 - val_loss: 3086.8591\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3647.1545 - val_loss: 3085.9626\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3671.0798 - val_loss: 3108.1296\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3657.5632 - val_loss: 3108.3220\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3666.5938 - val_loss: 3090.1636\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3684.1182 - val_loss: 3102.6755\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3681.6262 - val_loss: 3083.9011\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3661.1633 - val_loss: 3073.5645\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3636.3555 - val_loss: 3075.1440\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3678.9785 - val_loss: 3084.1868\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3666.5039 - val_loss: 3096.8750\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3643.1250 - val_loss: 3116.0486\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3628.8940 - val_loss: 3090.7275\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3652.5596 - val_loss: 3094.2832\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3655.4971 - val_loss: 3067.2874\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3634.4648 - val_loss: 3152.6660\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3652.8582 - val_loss: 3153.1445\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3698.7063 - val_loss: 3134.5486\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3647.3562 - val_loss: 3124.3333\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3690.5149 - val_loss: 3152.9336\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3677.5879 - val_loss: 3110.1516\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3695.9788 - val_loss: 3146.0347\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3680.6108 - val_loss: 3105.3999\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3694.9805 - val_loss: 3072.2073\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3698.7654 - val_loss: 3056.5581\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3663.3074 - val_loss: 3122.4558\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3666.6709 - val_loss: 3075.5903\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3704.4915 - val_loss: 3082.1106\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3685.6553 - val_loss: 3144.6572\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3656.6653 - val_loss: 3096.5039\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3674.8481 - val_loss: 3069.9744\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3628.8652 - val_loss: 3101.0066\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3663.1108 - val_loss: 3114.7341\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3627.7224 - val_loss: 3077.1685\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3663.8274 - val_loss: 3156.6096\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3679.0603 - val_loss: 3107.2329\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3649.4438 - val_loss: 3139.7671\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3674.3513 - val_loss: 3145.8674\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3654.0115 - val_loss: 3100.5168\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3657.0356 - val_loss: 3063.4092\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3619.0906 - val_loss: 3061.4993\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3685.4146 - val_loss: 3066.9275\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3653.6484 - val_loss: 3115.3997\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3701.2227 - val_loss: 3039.2166\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3650.9956 - val_loss: 3135.3369\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3675.4763 - val_loss: 3084.6689\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3673.5601 - val_loss: 3050.8633\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3674.7012 - val_loss: 3050.1382\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3680.6284 - val_loss: 3071.3877\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3616.9487 - val_loss: 3117.2822\n",
      "Early stopping at epoch 162 due to no improvement in val_loss for 100 epochs.\n",
      "Models saved to C:\\Users\\kiran\\Documents\\_UIS\\sem6\\BACH\\DementiaMRI\\AE_ADJ_models\\newer_images\\public_alcoholism_paper_2D_ssim\\epoch_162 as 'public_alcoholism_paper_2D_ssim_autoencoder.keras', 'public_alcoholism_paper_2D_ssim_encoder.keras', 'public_alcoholism_paper_2D_ssim_decoder.keras'.\n",
      "Loss history saved to C:\\Users\\kiran\\Documents\\_UIS\\sem6\\BACH\\DementiaMRI\\AE_ADJ_models\\newer_images\\public_alcoholism_paper_2D_ssim\\epoch_162\\history.json\n",
      "Model details saved to C:\\Users\\kiran\\Documents\\_UIS\\sem6\\BACH\\DementiaMRI\\AE_ADJ_models\\newer_images\\public_alcoholism_paper_2D_ssim\\epoch_162\\configs.json\n",
      "Training complete in 1972.32s\n",
      "Training public_alcoholism_paper_2D_ssim on CPU\n",
      "Input shape: (988, 80, 96)\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2s/step - loss: 4533.9312 - val_loss: 3295.3301\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3801.5105 - val_loss: 3163.9016\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3696.7993 - val_loss: 3137.4216\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3705.3462 - val_loss: 3105.3149\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3690.7092 - val_loss: 3130.6401\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3700.5967 - val_loss: 3061.9126\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3674.7812 - val_loss: 3176.6074\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3681.9287 - val_loss: 3108.0583\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3679.0095 - val_loss: 3152.2539\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3652.2795 - val_loss: 3074.3784\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3651.6267 - val_loss: 3102.7520\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3681.2905 - val_loss: 3076.3582\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3609.0605 - val_loss: 3072.8384\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3679.2358 - val_loss: 3086.2783\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3654.2559 - val_loss: 3121.7588\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3668.0227 - val_loss: 3102.9985\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3640.3970 - val_loss: 3098.1702\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3675.9438 - val_loss: 3145.0271\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3638.8843 - val_loss: 3151.6472\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3688.5422 - val_loss: 3150.7568\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3665.4375 - val_loss: 3102.2068\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3703.7476 - val_loss: 3111.8677\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3670.5293 - val_loss: 3165.0850\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3676.7800 - val_loss: 3077.1938\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3657.5713 - val_loss: 3137.4573\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3675.1650 - val_loss: 3087.7676\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3663.5654 - val_loss: 3112.0713\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3669.0942 - val_loss: 3104.7761\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3696.7793 - val_loss: 3065.3792\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3691.4238 - val_loss: 3138.7178\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3675.7649 - val_loss: 3079.6501\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3648.4812 - val_loss: 3059.9248\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3660.8337 - val_loss: 3054.6279\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3651.5208 - val_loss: 3101.3511\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3654.2998 - val_loss: 3146.2673\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3709.1431 - val_loss: 3137.8579\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3664.4282 - val_loss: 3077.7822\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3676.4858 - val_loss: 3027.2449\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3656.1711 - val_loss: 3077.8132\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3654.5317 - val_loss: 3120.8125\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3693.3110 - val_loss: 3138.2136\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3639.4502 - val_loss: 3116.5269\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3678.2622 - val_loss: 3083.0408\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3658.6968 - val_loss: 3040.1409\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3684.8479 - val_loss: 3113.6162\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3666.3699 - val_loss: 3141.4097\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3688.3389 - val_loss: 3097.3123\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3722.2075 - val_loss: 3144.7522\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3653.3330 - val_loss: 3091.6106\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3656.5227 - val_loss: 3094.8242\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3670.9878 - val_loss: 3108.5012\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3650.4673 - val_loss: 3153.0754\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3692.0476 - val_loss: 3094.9021\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3690.8423 - val_loss: 3114.6426\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3698.4036 - val_loss: 3171.0945\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3658.3591 - val_loss: 3162.7751\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3686.3679 - val_loss: 3070.3188\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3647.2451 - val_loss: 3049.0115\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3664.9624 - val_loss: 3172.0161\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3652.5569 - val_loss: 3090.6538\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3670.5000 - val_loss: 3120.9443\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3639.7783 - val_loss: 3096.5315\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3666.8271 - val_loss: 3097.5452\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3671.1929 - val_loss: 3109.6743\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3647.3555 - val_loss: 3074.5039\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3704.2153 - val_loss: 3088.2061\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3675.3765 - val_loss: 3088.0537\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3630.7039 - val_loss: 3105.5115\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3682.0171 - val_loss: 3089.5374\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3690.7739 - val_loss: 3083.6365\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3656.1604 - val_loss: 3138.4407\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3670.6511 - val_loss: 3104.1724\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3680.1770 - val_loss: 3089.5215\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3684.5781 - val_loss: 3046.6904\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3655.0632 - val_loss: 3074.5222\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3674.6418 - val_loss: 3044.8381\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3684.0896 - val_loss: 3083.3577\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3705.3440 - val_loss: 3082.2869\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3659.5759 - val_loss: 3082.7046\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3722.1584 - val_loss: 3098.8037\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3651.9546 - val_loss: 3083.5601\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3627.0669 - val_loss: 3106.5518\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3663.1936 - val_loss: 3134.3350\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3668.9421 - val_loss: 3112.3342\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3684.6873 - val_loss: 3129.3586\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3665.0298 - val_loss: 3114.7461\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3646.4697 - val_loss: 3121.0193\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3679.5088 - val_loss: 3086.6279\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3670.3037 - val_loss: 3035.4243\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3667.3926 - val_loss: 3093.0540\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3665.9731 - val_loss: 3061.7202\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3659.2107 - val_loss: 3121.1106\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3672.7007 - val_loss: 3070.0984\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3668.1372 - val_loss: 3087.2544\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - loss: 3679.1145 - val_loss: 3121.7324\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - loss: 3706.1016 - val_loss: 3125.4619\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3695.8535 - val_loss: 3074.0581\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3697.5881 - val_loss: 3076.6780\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3664.8645 - val_loss: 3073.2617\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3647.0330 - val_loss: 3114.3682\n",
      "Models saved to C:\\Users\\kiran\\Documents\\_UIS\\sem6\\BACH\\DementiaMRI\\AE_ADJ_models\\newer_images\\public_alcoholism_paper_2D_ssim\\epoch_100 as 'public_alcoholism_paper_2D_ssim_autoencoder.keras', 'public_alcoholism_paper_2D_ssim_encoder.keras', 'public_alcoholism_paper_2D_ssim_decoder.keras'.\n",
      "Loss history saved to C:\\Users\\kiran\\Documents\\_UIS\\sem6\\BACH\\DementiaMRI\\AE_ADJ_models\\newer_images\\public_alcoholism_paper_2D_ssim\\epoch_100\\history.json\n",
      "Model details saved to C:\\Users\\kiran\\Documents\\_UIS\\sem6\\BACH\\DementiaMRI\\AE_ADJ_models\\newer_images\\public_alcoholism_paper_2D_ssim\\epoch_100\\configs.json\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3654.0630 - val_loss: 3114.7349\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3670.4199 - val_loss: 3101.7227\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3673.2034 - val_loss: 3057.6992\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - loss: 3664.4492 - val_loss: 3066.9438\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3644.8474 - val_loss: 3140.1938\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3684.5596 - val_loss: 3072.9819\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3683.3071 - val_loss: 3082.8899\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3662.6865 - val_loss: 3088.3792\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3650.3401 - val_loss: 3101.6836\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3666.9177 - val_loss: 3074.1279\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3681.2681 - val_loss: 3102.2922\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3652.1106 - val_loss: 3115.3157\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3673.7383 - val_loss: 3082.6216\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3658.3228 - val_loss: 3125.9614\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3657.5786 - val_loss: 3092.0764\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3646.8684 - val_loss: 3124.0115\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3643.3582 - val_loss: 3157.4658\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3673.2441 - val_loss: 3114.0627\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3640.7898 - val_loss: 3112.2439\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3689.1892 - val_loss: 3111.3770\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3669.1099 - val_loss: 3071.9011\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3680.5498 - val_loss: 3054.4470\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3668.0906 - val_loss: 3090.9539\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3672.8059 - val_loss: 3066.6638\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3659.3943 - val_loss: 3142.0681\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3656.4360 - val_loss: 3099.2749\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3650.2268 - val_loss: 3087.7080\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3703.2358 - val_loss: 3091.1448\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3657.1875 - val_loss: 3089.1995\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3680.6794 - val_loss: 3105.3706\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3642.7822 - val_loss: 3104.5625\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3655.9426 - val_loss: 3133.2949\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - loss: 3643.2473 - val_loss: 3066.2573\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3649.3640 - val_loss: 3094.7126\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3672.5601 - val_loss: 3129.9546\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 3654.8999 - val_loss: 3094.6631\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 3669.6304 - val_loss: 3095.1685\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - loss: 3640.6787 - val_loss: 3127.8560\n",
      "Early stopping at epoch 138 due to no improvement in val_loss for 100 epochs.\n",
      "Models saved to C:\\Users\\kiran\\Documents\\_UIS\\sem6\\BACH\\DementiaMRI\\AE_ADJ_models\\newer_images\\public_alcoholism_paper_2D_ssim\\epoch_138 as 'public_alcoholism_paper_2D_ssim_autoencoder.keras', 'public_alcoholism_paper_2D_ssim_encoder.keras', 'public_alcoholism_paper_2D_ssim_decoder.keras'.\n",
      "Loss history saved to C:\\Users\\kiran\\Documents\\_UIS\\sem6\\BACH\\DementiaMRI\\AE_ADJ_models\\newer_images\\public_alcoholism_paper_2D_ssim\\epoch_138\\history.json\n",
      "Model details saved to C:\\Users\\kiran\\Documents\\_UIS\\sem6\\BACH\\DementiaMRI\\AE_ADJ_models\\newer_images\\public_alcoholism_paper_2D_ssim\\epoch_138\\configs.json\n",
      "Training complete in 1724.42s\n"
     ]
    }
   ],
   "source": [
    "for m in models_list: #[A2,A4,A5]\n",
    "    m.train(groups[\"CN\"], epochs=1000, batch_size=128, verbose=True, \n",
    "    save_path=r\"C:\\Users\\kiran\\Documents\\_UIS\\sem6\\BACH\\DementiaMRI\\AE_ADJ_models\\newer_images\", \n",
    "    save_interval=100, patience=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01c036c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Test to see if loading works\n",
    "# models_list = [AE.open(r\"C:\\Users\\kiran\\Documents\\_UIS\\sem6\\BACH\\DementiaMRI\\AE_ADJ_models\\newer_images\\public_repo\\epoch_9\")]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
